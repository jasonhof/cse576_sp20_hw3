2.2.1 Question
Q: Why might we be interested in both training accuracy and testing accuracy? What do these two numbers tell us about our current model?
A: The training and testing accuracy, and the rate they are changing on each pass relative to each other, can tell us about the way our model is currently fitting the data and can give us hints about its future performance.  For example, if we get a 

2.2.2 Question
Q: Try varying the model parameter for learning rate to different powers of 10 (i.e. 10^1, 10^0, 10^-1, 10^-2, 10^-3) and training the model. What patterns do you see and how does the choice of learning rate affect both the loss during training and the final model accuracy?
A: When varying the learning rate, there seems to be a "sweet spot" somewhere in the middle of that range that allows for the best balance of model convergence time and accuracy.  At the highest learning rate I tested (10^1), the model performs with poor accuracy at the end (<10% training and test accuracy), and the loss quickly becomes nan after 4 iterations.  As we decrease the learning rate (to 10^0), we get a much better accuracy at the end (~89.9% training and test accuracy).  We get slightly better accuracy with slower learning rates (91.7% with 10^-1 and 90.2% with 10^-2), but the time required to converge differs: the first iteration to reach over 90% Batch Accuracy is Iteration 22 for 10^-1 vs Iteration 95 for 10^-2.  This pattern is inversely correlated with the Training Loss, as well.  The higher learning rate in 10^-1 results in a faster decline in Training Loss (from 2.3 in Iteration 0 to .37 in Iteration 22).  In 10^-2, the Training Loss declines more slowly (from 2.3 in Iteration 0 to 1.2 in Iteration 22).  This contrasts with the Training Loss from 10^1, which increases from 2.3 in Iteration 0 to 62.5 in Iteration 3, followed by 'nan', indicating that the Training Loss has continued increasing beyond the capacity of the data type to store it.  The final thing to note is that when the learning rate gets too small, the model takes longer to converge.  For example, for a learning rate of 10^-3, the model still has not achieved 90% test accuracy by Iteration 1000, and the Loss is still above .5, whereas the larger learning rates resulted in a loss of less than .5 by Iteration 1000 (except for 10^1 and 10^0, which do not seem to be able to get below a .5 loss at any iteration, as they may be oscillating the model too far each way).

2.2.3 Question
Q: Try varying the parameter for weight decay to different powers of 10: (10^0, 10^-1, 10^-2, 10^-3, 10^-4, 10^-5). How does weight decay affect the final model training and test accuracy?
A: For these questions, I kept the model learning rate steady at 10^-2.  As I varied weight decay along the suggested values, I noticed that the final accuracy of the model increased with decreasing weight decay, except for the final, smallest value (10^-5), which slightly decreased the final training accuracy by .00005 (test accuracy remained the same as 10^-4).  With the large values for weight decay (10^0 and 10^1), the final training and test accuracy of the model were lower (~77% and ~86%, respectively).  Also, for those high values, the Loss remained relatively high at the 1000th iteration, and the Batch accuracy remained relatively low, indicating that the model had not fully converged to its optimal fit for the training set.  With the highest setting for decay (10^0), the model still did not converge more after 10000 iterations, indicating that the weights had decayed to the point where there wasn't much more room to improve after 1000 iterations.

2.3.1 Question
Q: Currently the model uses a logistic activation for the first layer. Try using all the other activation functions we programmed. How well do they perform? What's best?
A:  It appears that all activations for the first layer perform fairly well, achieving greater than 88% accuracy on both the training set and the test set after 1000 iterations.  The logistic layer actually performs the worst of all the layers we implemented, achieving 88.4% training accuracy and 88.9% test accuracy.  The best performance was achieved by Tanh, which showed a 92.4% training accuracy and 92.5% test accuracy. Linear, Relu and Leaky Relu all achieved between 91-92% accuracy on both the training set and the test set.  The rate was kept at .01 for this testing.

2.3.2 Question
Q: Using the same activation, find the best (power of 10) learning rate for your model. What is the training accuracy and testing accuracy?
A: I used Tanh activation for these tests, as it achieved the best accuracy at a learning rate of .01.  I based my observation on the test accuracy result, as that is indicative of how well the model fits new data that may come in the future.  When training for 1000 iterations, a learning rate of .1 appeared to be best, achieving a test accuracy of 95.37%.  Increasing or decreasing the learning rate from .1 to 1 or .01 reduced the test accuracy in my tests.

2.3.3 Question
Q: Right now the regularization parameter `decay` is set to 0. Try adding some decay to your model. What happens, does it help? Why or why not may this be?
A: Setting the Activation to Tanh, the iterations to 1000 and the learning rate to .1, adding a small amount of decay did help to increase the test accuracy at the end of a run.  I found that a decay rate of .0001 improved the test accuracy to 95.44% from 95.37%.  However, taking this too far (to a decay of .001 or .01), resulted in a lower test accuracy.  This is likely due to the effect of the decay- as we get to higher iterations, we are getting closer to our local minimum or fit for our model, and we don't want the model weights to jump too much.  Therefore, some decay allows us to stay very close to the local minimum.  However, too much decay causes us to decay too quickly, and the early changes to weights don't allow us to reach our best fit point in the model, even at the end of 1000 iterations.  It seems to be a delicate balancing act!

2.3.4 Question
Q: Modify your model so it has 3 layers instead of 2. The layers should be `inputs -> 64`, `64 -> 32`, and `32 -> outputs`. Also modify your model to train for 3000 iterations instead of 1000. Look at the training and testing accuracy for different values of decay (powers of 10, 10^-4 -> 10^0). Which is best? Why?
A: Continuing to use TANH, building in 3 layers instead of 2, setting iterations to 3000, keeping the learning rate at .1, I noticed that the resultant test accuracy was higher with a decay of 0 than it was for any settings with 2 layers.  The test accuracy with a decay of 0 was 96.9%, while the training accuracy was 98.5%.  When I added a little bit of decay (.0001), the test accuracy (97.1%) increased and the training accuracy (98.4%) came down a bit, indicating that we had a more accurate model on the test set, and we were also not overfitting quite as much (due to the reduced difference between test accuracy and training accuracy).  However, increasing the decay any more than .0001 (to .001 or .01) reduced the test accuracy (to 96.6% and 90.6%, respectively).  By the time we get up to high levels of decay (.1 or 1), the test accuracy becomes low (74.9% and 9.8%), indicating that we are decaying our learning rate too quickly for this model.  This indicates that the decay of .0001 was the best setting for this model and these other hyperparameters.

2.3.5 Question
Q: Modify your model so it has 4 layers instead of 2. The layers should be `inputs -> 128`, `128 -> 64`, `64 -> 32`, and `32 -> outputs`. Do the same analysis as in 2.3.4.
A: Continuing to use TANH, building in 4 layers instead of 3, setting iterations to 3000, keeping the learning rate at .1, I noticed that we were able to achieve a slightly greater accuracy of 97.09% with no decay (as compared to the maximum for 3 layers of 97.07%).  Increasing the decay (to .0001 and .001) for this model only reduced the test accuracy (to 97.00% and 96.72%, respectively), in a similar manner to the 3-layer model above.  At this point, it is also noteworthy that the training takes significantly longer (perhaps 2x longer, as an estimate) than in previous tests, as it appears that each iteration takes longer to forwardpropagate and then backpropagate the loss.  This makes sense, as there are more layers, but it's worth mentioning that the increase in accuracy is paid for by more computation time.

2.3.6 Question
Q: Use the 2 layer model with the best activation for layer 1 but linear activation for layer 2. Now implement the functions `l1_loss` and `l2_loss` and change the necessary code in `classifier.cpp` to use these loss functions. Observe the output values and accuracy of the model and write down your observations for both the loss functions compared to cross-entropy loss. P.S. L2 and L1 losses are generally used for regression, but this is a classification problem.
A: When switching to L2 and L1 loss for this classification problem, but keeping the learning rate at .1, iterations at 3000, and setting up a 3-layer model with TANH for the first layer, LINEAR for the second layer and SOFTMAX for the output layer, I saw that L2 loss created a more accurate model, with a 96.46% test accuracy, compared with 87.50% accuracy for L1 loss.  Cross-entropy loss was still slightly more accurate with the TANH model from the previous question, coming in at 97.07% test accuracy.

3.2.1 Question
Q: How well does your network perform on the CIFAR dataset?
A: I am not able to get good performance with my network on the CIFAR dataset.  After trying 3 different network configurations and 20+ different combinations of hyperparameters, the highest accuracy I was able to get was 46.1% test accuracy with these settings: 3 layers (TANH, LINEAR, SOFTMAX), .01 learning rate, .00001 decay, and CROSS_ENTROPY loss function.  Perhaps this lack of accuracy is due to the lack of diverse non-linearities in our neural network implementations (such as dropout), which are helpful in more complex tasks and avoid the simplification of our model to a linear model.
